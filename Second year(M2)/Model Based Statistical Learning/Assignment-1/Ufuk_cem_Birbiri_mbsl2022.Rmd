---
title: "Statistical learning project"
author: "Ufuk Cem Birbiri"
date: "15.11.2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment-1

The goal is to apply EM for multivariate GMMs on the Wine dataset, available in the `pgmm` package.

### To do:

#### 1) Implementing the EM

- Implement from scratch the EM algorithm for a GMM on the variables 2 and 4 of the wine data set.
- Cluster the data and compare your results with k-means.
- To assess the quality of the clustering, you may use the function classError and/or adjustedRandIndex from the Mclust package.


#### 2) Model selection

- Try to find a relevant number of clusters using the three methods seen in class: AIC, BIC, and (cross-)validated likelihood.

#### 3) Towards higher dimensional spaces

- Try to model more than just two variables of the same data set. Do you find the same clusters, the same number of clusters.

---------------------------------

## Install libraries


```{r, include=FALSE}
#install.packages("pgmm")
library(pgmm)
#install.packages("mclust")
library(mclust)

#install.packages("ggplot2")
library(ggplot2)
#install.packages("expm")
library(expm)

#install.packages("scatterplot3d") # Install
library(scatterplot3d) # load
```

### 1.2. Wine dataset

From the pgmm package are loaded, we can load the **wine** data in the environment.
In the first 2 parts, we just work on the variables 2 and 4 of the wine data set (Alcohol and Fixed Acidity). We also load the Type of wine for evaluating the cluster result.



```{r}
data(wine)
X = as.matrix(wine[,c(2,4)])
y = wine[,1]

print("What is the dimensoins of the data?")
print(paste0("Dimension of X: " , nrow(X), ", " ,ncol(X)))
print(paste0("Dimension of y: " , nrow(y), ", " ,ncol(y)))
```

We visualize the data points:
```{r}
plot(X,col=y)

```

## 2. Implementation of EM 
### useful_functions.R

Below fucntions are adapted from useful_functions.R:
```{r}
##############################################################################
# The logarithm of the sum of the exponentials of the arguments
logsumexp <- function (x) {
  y = max(x)
  y + log(sum(exp(x - y)))
}
##############################################################################

##############################################################################
#Normelize the arguments
normalise <- function (x) {
  logratio = log(x) - logsumexp(log(x))
  exp(logratio)
}
##############################################################################

log_gaussian_density <- function (x, mean, sigma) {
  distval <- mahalanobis(x, center = mean, cov = sigma)
  logdet <- determinant(sigma,logarithm = TRUE)$modulus
  logretval <- -(ncol(x) * log(2 * pi) + logdet + distval)/2
  return(logretval)
}

##############################################################################
##############################################################################
##############################################################################
# Below function generates some centroids according to data(X). The explanation is inside the function.
generate_centroids_for_mu <- function(num_centroids = 3, X){
  #INPUT:
  # num_centroids= nnumber of centroids to generate
  # X = the data
  
  #Set seed to avoid randomization:
  set.seed(33)
  n_col = ncol(X)
  n_row = nrow(X)
  #Create an empty matrix:
  centroids = matrix(ncol=n_col, nrow=num_centroids)

  for (i in 1:n_col) {
    #For each column, generate 3 uniform random numbers between a and b:
    #a = min value of data column
    #b = max value of data column
    centroids[,i] = runif(num_centroids, min(X[,i]), max(X[,i]))
  }
  #Conver it to a dataframe:
  centroids = data.frame(centroids, stringsAsFactors = FALSE)
  return(as.matrix(centroids))
}

##############################################################################
```



### Create function to get computations on Log-likelihood

```{r}
metrics_computation <- function(X, K, prob, mu, sigma) {
  n = nrow(X)
  d = ncol(X)
  log_gamma_numerator = matrix(unlist(lapply(1:K, FUN = function(k) {log(prob[k]) + dmvnorm(X, mu[k,], sigma[[k]], log = TRUE)})), n, K)
  log_likelihood = sum(apply(log_gamma_numerator , 1, logsumexp))
  return (list(logLik=log_likelihood))
}
```


### Implementation of EM
In the EM algorithm our goal is to predict these parameters:
$$\theta = (\pi_1,...,\pi_K, \mu_1,...\mu_K, \Sigma_1,...\Sigma_K)$$

E-Step: For all n, k, we compute $$\gamma(z_{nk}) = p(k|x_n) = \frac{\pi_k \mathcal{N}(x_n|  \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_n|  \mu_j, \Sigma_j)}$$

which is called responsibilities.Once we have the responsabilities, we will cycle through the three equations that will collectively constitute our M-step:
$$\mu_k^{'} = \frac{1}{N_k}\sum_{j=1}^{n}\gamma(z_{nk})x_n$$

$$\pi_k^{'} = \frac{\sum_{i=1}^{N}Pr(u_i=k \left\vert x_i\right.; \mu, \sigma, \pi)}{N} = \frac{N_k}{N}
$$


$$\Sigma_k^{'} = \frac{1}{N_k}\sum_{j=1}^{n}\gamma(z_{nk})(x_n - \mu_k)(x_n -\mu_k)^{T}$$


After each iteration of the EM algorithm (one E step followed by one M
step), we can monitor convergence by looking at the evolution of the value
of the log-likelihood which will be hopefully be maximised at the end.





**EM_algorithm**

Let's implement it!
The input and output value information is given inside the function:

```{r}
EM_algorithm <- function(X, K, max_it=50, showits=T){
  ###############
  #### INPUTS:
  # X = data
  # K = number of clusters
  # max_it = Maximum iteration before the EM algorithm stops
   ###############
  #### OUTPUTS:
  # prob = probability vector of clusters = pi_k
  # mu = list of mu values = mu_k
  # sigma = list of sigma values = (sigma_k)
  # gamma = list of gamma values = gamma(z_{nk})
     ###############
  

  #We set a tolerance value for early-stopping:
  tolerance = 1e-8
  
  #Let's initialize of the mu, sigma, and gamma
  prob = rep(1/K, K)
  mu = generate_centroids_for_mu(K, X)
  sigma = lapply(1:K, function(i) cov(X))
  gamma = matrix(NA, nrow(X), K)
  
  log_likelihood = 0
  for  (i in 1:max_it) {
    #We store the old value of log-lik and probability to compare them with
    #the new ones later
    log_likelihood_old = log_likelihood
    prob_old = prob
    ####################################################
    #E step:
    log_gamma_numerator = matrix(nrow=nrow(X), ncol = K)
    for (k in 1:K){
      log_gamma_numerator[,k] = log(prob[k]) + dmvnorm(X, mu[k,], sigma[[k]], log = TRUE)
    }
    ## Normalize the gamma:
    log_gamma_denominator = apply(log_gamma_numerator , 1, logsumexp)
    log_gamma = log_gamma_numerator - log_gamma_denominator
    gamma = exp(log_gamma)
    ####################################################
    # M step
    for (k in 1:K){
      #Calculate the parameters:
      nk = sum(gamma[,k])
      prob[k] = nk / nrow(X)
      mu[k,] = colSums(gamma[,k]*X) / nk
      sigma[[k]] = ( t(sweep(X, 2, mu[k,])) %*% diag(gamma[,k]) %*% (sweep(X, 2, mu[k,])) ) / nk 
    }
    
    # Evaluate the log-likelihood:
    log_likelihood = sum(log_gamma_denominator)
    ### We compare the old and new parameters:
    old_parameters =  c(log_likelihood_old, prob_old)           
    new_parameters = c(log_likelihood, prob)
    #Print the log-likelihood
    if (showits & (i == 1 | i%%5 == 0))         
      cat(paste("Iterations ", format(i),  " of EM: ", " ------ loglik : ", log_likelihood,"\n", sep = ""))
    #We test the tolerance for early-stopping:
    if(min(abs(old_parameters - new_parameters)) <= tolerance){
      break
    }
  }
  # compute the sum log-likelihood
  metrics = metrics_computation(X, K, prob, mu, sigma)
  logLik <- metrics$logLik
  
  return (list(prob=prob, mu=mu, sigma=sigma, gamma=gamma, logLik=logLik))

}
```


After the EM algorithm implementation, we need to find the clusters from the learned parameters. The following function gives us the clusters of each data point.


```{r}
# Input param is gamma from EM model result
EMOrigin.findCluster <- function(gamma){
  ###############
  #### INPUT:
  # gamma = learned parameter from the EM algorith
   ###############
  #### OUTPUT:
  # clusters = list of clusters for each data point in X
     ###############
  
  
  clusters = rep(NA, nrow(gamma))
  # For each rows, we find order of value gamma where have the max gamma value
  for (i in 1:nrow(gamma)){
    #Find the maximum
    clusters[i] = which.max(gamma[i,])
  }
  return (clusters)
}
```



### K-mean Algorithm implement

In K-Means, we first declare the centroids(cluster centers) and the max number of iterations. Then we assign each data point in the dataset to a cluster center. Then we update the cluster centers by taking the average of the data points in each cluster. This procedure is done until we reach the maximum number of iterations.


```{r}
#Calculate the distance:
measure_distance <- function(x, y){
  return(sum((x-y)^2))
}


kmeans=function(data,K=4){
  #Set the maximum iteration as 250
   max_iteration = 250
  p = ncol(data)
  centroids=data[sample.int(nrow(data),K),]

  cluster=rep(0, nrow(data))
  should_stop = 0 
  iteration=1
  
  for (iteration in 1:max_iteration){
    old_centroids=centroids
    # For each data point, measure the distance to each centroid,
    #then assigning each point to a centroid:
    for (i in 1:nrow(data)){
      min_dist=1e10
      for (centroid in 1:nrow(centroids)){
        distance_to_centroid = measure_distance(centroids[centroid,], data[i,])
        #If the distance is less than the min distance, we can assign it
        if (distance_to_centroid<=min_dist){
          cluster[i]=centroid
          min_dist=distance_to_centroid}
        }}
    ##Assigning each point to a centroid
    for (i in 1:nrow(centroids)){
      centroids[i,]=apply(data[cluster==i,],2,mean)
    }}
  return(list(size=tabulate(cluster),cluster=cluster,centers=centroids))
}
```
I determined the maximum iteration as 250 because at some point the K-Means algorithm should converge and stop.

## Comparing EM and K-Means results

We compares two different methods of the unsupervised classification. EM Algorithm have similarity with Kmeans, because they use the same optimization strategy on the M Step algorithm.



In EM case (with mixture of Gaussians) , each Gaussian has an associated mean and covariance matrix. The parameters are initialized by randomly selecting means (like k means), then the algorithm converges on a locally optimal solution by iteratively updating values for means and variance.

K-means finds the parameters of the centroid to minimize X minus the mean squared.
The EM model finds the centroid to minimize X minus mean squared over the standard deviation squared.
Mathematically the difference is the denominator $\sigma^2$, which means EM Algorithm takes variance into consideration when it calculates the measurement.

That's why K-Means will assign each data points to exactly one cluster but EM Algorithm will assign data points to cluster with some probability.

EM it updates the mean, covariance matrix, and the probability vector of clusters. 

### EM algorithm result

```{r}
#Let's run the EM and see how it converges.
params = EM_algorithm(X, 3, 500, showits = T)
```

Accoroding to EM result, the convergence point of the EM is -949.7 after 165 iteration. I have put a tolerance limit in the EM algorithm as 1e-8. If the minimum absolute difference between the new and old parameters are less than 1e-8, than the EM algoritmn stops because it will not converge again. 



We can see the parameters i.e prob, mu, sigma, and gamma parameters of the EM model by running below cell. It gives us the learned parameters after the training. I put it in a comment because printing all the parameters takes a lot of space in the R-Markdown. To see the parameters, please put it out of the comment:
```{r}
#params
```

### Investigating the EM parameters:

First, let's look at the prob, which is a probability vector of the cluster percentages.
```{r}
params$prob
```
We see that cluster 1 has the 51.8% of the data, cluster 2 has 15.8% and cluster 3 has 32.2%. It is not a balanced dataset.


Let's see the mu parameters estimation of EM:
```{r}
params$mu
```
The mu parameter shows the centroids of the clusters detected by the EM algorithm. The first component of the mu parameter (X1 above) represents the average alcohol concentration of the cluster. The second component (X2) represents the average Acidity of the cluster. Now, we print the mean values of Acidity and alcohol concentration columns to compare with the cluster centers.


```{r}
print(paste0("The average Alcohol concentration of the whole dataset: ", mean(as.matrix(wine[,c(2)]))))
print(paste0("The average Acidity of the whole dataset: ", mean(as.matrix(wine[,c(4)]))))
```
 The average value of the second cluster's center is 13.63 which has the most alcohol concentration of wine also the more acidic(X2= 108.15).


### Visualize the result:

Now, we plot the cluster results on X data.

```{r}
cluster_result = EMOrigin.findCluster(params$gamma)
#Plot them:
plot(X, col=cluster_result, pch=20)
points(params$mu, col = 1:3, pch = 10, cex=2)
```


The EM results is not very similar to the original data points visually. The EM algorithm divided the data points by mostly considering the acidity and alchol concentration as it seems in the result. 

**Now we willplot the result. There will be two ellipses around each cluster: multivariate t-distribution(solid line) and normal distribution (dotted line)**



```{r}
#Plotting function:
plot_cluster_area <- function(X, centroids, cluster_result){
  x_df = data.frame(X)
  mu_clusters = data.frame(centroids)
  names(mu_clusters) = names(x_df)
  colors = as.factor(cluster_result)
  ggplot(x_df, aes_string(names(x_df)[1], names(x_df)[2], color=colors)) + 
      geom_point() +
      stat_ellipse(geom="polygon", aes(fill=colors), alpha=0.15, type = "norm", linetype = 2) + 
    labs(color="Clusters") +
    stat_ellipse(geom="polygon", aes(fill=colors), type = "t", alpha=0.15) +guides(fill = "none") + geom_point(data=mu_clusters, color=c(2,3,1), shape=10, size=3)
}

#Plot them:
plot_cluster_area(X, params$mu, cluster_result)
```



Now it is time to experiment K-Means and see the result:

### K-Means algorithm result

```{r}
kmeans_result <- kmeans(X, 3)
kmeans_result
```
**Comparing results of KMeans and EM:**
```{r, echo = FALSE}
print("Data distributions with 3 clusters:")
print("Kmeans :")
print(paste0((kmeans_result$size)/sum(kmeans_result$size)))
print("GMM :")
print(paste0((params$prob)))
```

The quantities look similar, however the cluster 1 and cluster 2 seems rapleced in EM and KMeans result. 


```{r}
kmeans_result$centers
```

Accroding to KMeans result, the most acidic cluster seems cluster 1 (it was cluster 2 in the EM result.). To analyse it better, let's plot the KMeans result:

###  Plotting the K-Means result

```{r}
plot(X, col = kmeans_result$cluster,  pch = 20)
points(kmeans_result$centers, col = 1:3, pch = 10, cex=2)
```

It seems like the EM algorithm has a better result than KMeans. Kmeans mostly clustered the data points according to acidity, not alcohol content because all clusters are horizontal. We can plot the Kmeans results with drawing ellipses around cluster centers such as multivariate t-distribution(solid line) and normal distribution (dotted line), respectively.

```{r}
plot_cluster_area(X, kmeans_result$centers, kmeans_result$cluster)
```
Still, the EM results seems more correct and similar to real clusters than Kmeans result. We can test their performances by splitting the data as train-test set and measure their performance:

```{r}
## 70% of the sample size
smp_ratio = 0.70
smp_size <- floor(smp_ratio * nrow(X))
## set the seed to make your partition reproducible
set.seed(35)

# We randomize the dataset via its index
randomized_indexes = sample(1:nrow(X))
randomized_X = X[randomized_indexes,]
randomized_y = y[randomized_indexes]


n = dim(X)[1]
#Round it to have integer numbers
training_split = round(0.70*n,0)

x_train <- randomized_X[1:training_split,]
x_test <- randomized_X[(training_split+1):n,]
y_train <- randomized_y[1:training_split]
y_test <- randomized_y[(training_split+1):n]
```

We can plot the test data to see whether the class members distributed equally:



```{r}
print(paste0("Number rows of testing dataset: " , nrow(x_test)))
plot(x_test,col=y_test, pch=20, main="Testing data")
```


**Using train and test data**

- Train data using KMeans and EM:

```{r}
#EM:
train_EM_result = EM_algorithm(x_train, 3, 1000, showits = F)
train_EM_result$cluster = EMOrigin.findCluster(train_EM_result$gamma)

#KMeans:
train_Kmeans_result = kmeans(x_train, 3)
```

- Test data using KMeans and EM:

```{r}
#EM::
test_EM_result = EM_algorithm(x_test, 3, 1000, showits = F)
test_EM_result$cluster = EMOrigin.findCluster(test_EM_result$gamma)

#KMeans:
test_KMeans_result = kmeans(x_test, 3)
```

###  ERROR ANALYSIS

We are gonna use the classError function from the Mclust package to asses the error in both models.

**EM - Training error**

```{r}
classError(train_EM_result$cluster, y_train)
```

**KMeans - Training error**

```{r}
classError(train_Kmeans_result$cluster, y_train)
```
The EM class error rate is 48.8%, and the KMeans class error rate is 49.6%. EM algorithm seems perform better. 


**EM - Testing error**:

```{r}
classError(test_EM_result$cluster, y_test)
```

**KMeans - Testing error**

```{r}
classError(test_KMeans_result$cluster, y_test)
```


Accroding to results EM class error rate on train data(0.488) and on test data(0.339) is smaller than the KMeans training class error(0.496) and the test error(0.452). 


####  Adjusted Rand Index analysis

When the Adjusted Rand score is closer to 1, the better the performance of the clustering algorithm. Let's see the results:

**EM - Training error**

```{r}
adjustedRandIndex(train_EM_result$cluster, y_train)
```

**KMeans - Training error**

```{r}
adjustedRandIndex(train_Kmeans_result$cluster, y_train)
```
We see that the KMeans adjusted Rand Index(0.139) on training data is more closer to 1 than the EM(0.123), that means KMeans is better on training dataset.

**EM - Testing error**

```{r}
adjustedRandIndex(test_EM_result$cluster, y_test)
```

**KMeans - Testing error**


```{r}
adjustedRandIndex(test_KMeans_result$cluster, y_test)
```

The EM adjusted Rand Index(0.315) on testing data is more closer to 1 than the KMeans(0.178), that means EM has better performance on testing dataset.

In overall, the EM algorithm has better performance than the KMeans. Kmeans only considers the acidity of the data points so it gets worse performance. 
## Selecting best number of clusters using AIC; BIC; and Cross-validate K Fold


We will try to find a relevant number of clusters using AIC, BIC, and Cross-validated K-Fold


 


### 1. AIC score

AIC score is calculated using the model parameters.
$$\text{number of } \pi_k = (K-1)$$

$$\text{number of } \\mu_k = (Kxd)$$

$$\text{number of } \Sigma_k = K \times \frac{d(d+1)}{2}$$


$$\text{ model parameters} = \text{number of } \pi_k + \text{number of } \mu_k + \text{number of } \Sigma_k$$
                  
                  


  $$AIC = log(\mathcal{L}(y | x) )-  \text{ model parameters}$$

```{r}
AIC_score <- function(loglikelihood, K, data) {
  n = nrow(data)
  d = ncol(data)
  num_mu = K * d
  num_sigma = K*d*(d+1)/2
  num_pi = K-1
  num_parameters = num_mu + num_sigma +num_pi
  return (loglikelihood - num_parameters)
}
AIC_EM <- function(data, min_k, max_k){
  results = c()
  #For every cluster number in the range(min_k: max_k), we will calculate the aic score
  for (i in min_k:max_k){
    em_result = EM_algorithm(data, i, showits=F)
    aic_score = AIC_score(em_result$logLik, i, data)
    results = append(results, aic_score)
    print(paste("Number of clusters =  ", i, " aic score = ",round(aic_score, 3)))}
  
  #Find best cluster number
  rangee= min_k:max_k
   maxx = which.max(results)
   best_k = rangee[maxx]
  print(paste("The best K-value is ", best_k , " clusters using AIC"))
  return (list(result=results,bestK= which.max(results)))
}
```


### 2. BIC score
The BIC score is calculated similar to AIC score.

$$\text{ model parameters} = \text{number of } \pi_k + \text{number of } \mu_k + \text{number of } \Sigma_k$$
                  
                  


  $$BIC = log(\mathcal{L}(y | x) )-  \frac{1}{2}log(n)\times \text{ model parameters}$$ 
  where $n$ is the number of data points. The BIC score is calculated as:

```{r}
BIC_score <- function(log_likelihood, K, data) {
  #We use the formula as it is:
  n = nrow(data)
  d = ncol(data)
  num_mu = K * d
  num_sigma = K*d*(d+1)/2
  num_pi = K-1
  num_parameters = num_mu + num_sigma +num_pi
  return (log_likelihood - num_parameters *log(n) / 2.0)}

BIC_EM <- function(data, min_k, max_k){
results = c()
  #For every cluster number in the range(min_k: max_k), we will calculate the bic score
  for (i in min_k:max_k){
    em_result = EM_algorithm(data, i, showits=F)
    bic_score = BIC_score(em_result$logLik, i, data)
    results = append(results, bic_score)
    print(paste("Number of clusters =  ", i, " bic score = ",round(bic_score, 3)))}
  
  #Find best cluster number
  rangee= min_k:max_k
   maxx = which.max(results)
   best_k = rangee[maxx]
  print(paste("The best K-value is ", best_k , " clusters using BIC."))
  return (list(result=results,bestK= which.max(results)))
}
```


### 3. Cross-validated K-Fold

Now we will use Cross-validated K-Fold to measure the performance of the EM algorithm. We will shuffle the data first, then divide the dataset to K groups. Each time we will use one of the group as testing and the others as traning. Every group will be used as a testing group. We will use different number of clusters to find the best K. We do 8-fold cross validation.


```{r}
crossValidation <- function(data,clusters=2, folds=8) {
  #To make produce the same result for everyone, we put seed:
  set.seed(101)
  #Sample random selection on the data:
  rows <- sample(nrow(data))
  #Shuffle:
  shuffled_data = data[rows, ]
  n_train = dim(shuffled_data)[1]
  fold_indexes = split(c(1:n_train), 
                       ceiling(seq_along(c(1:n_train))/(n_train/folds)))
  k_fold_results = c()
  
  for (i in fold_indexes){
    x_train = shuffled_data[-i,]
    x_test = shuffled_data[i,]
    # EM result
    EM_result = EM_algorithm(x_train, clusters,0)
    test_results = metrics_computation(x_test, clusters, EM_result$prob,EM_result$mu, EM_result$sigma)
    k_fold_results = append(k_fold_results, test_results$logLik)
  }
  #The final result is the mean of the all validation results
  return(mean(k_fold_results))
  
}
KFold_cross_validation <- function(data, min_k, max_k){
  listNumberCluster=min_k:max_k
  # Variable declaration
  results = c()
  
  # Use for loop to compute the corresponding Cross_val score on each cluster parameter and push to a list
  for (i in min_k:max_k){
    cross_val_score = crossValidation(data, cluster=i, folds=8)
     #print(paste("Number of clusters =  ", i, " Cross-Validation Score = ",round(cross_val_score, 3)))
    results = append(results, cross_val_score)
  }
  #Find best cluster number
  rangee= min_k:max_k
   maxx = which.max(results)
   best_k = rangee[maxx]
  print(paste("The best K-value is ", best_k , " clusters using BIC."))
  return (list(result=results,bestK= which.max(results)))
 
}
```
Now we will see the AIC, BIC, and cross validation scores on the data:

**AIC on EM**


```{r}
aic_results = AIC_EM(X, 3, 10)
plot(3:10, aic_results$result, type = "b", pch = 19, 
     col = "red", xlab = "Number of cluster", ylab = "AIC Score", main="AIC results on clusters")
```

For AIC score the best cluster number is 5 since it has the lowest score.

**BIC on EM**
```{r}
bic_results = BIC_EM(X, 3,10)
plot(3:10, bic_results$result, type = "b", pch = 19, 
     col = "red", xlab = "Number of cluster", ylab = "BIC Score", main="BIC results on clusters")
```

For BIC score the best cluster number is 3 since it has the lowest score.


```{r}
crossval_results = KFold_cross_validation(X, 3,10)
plot(3:10, crossval_results$result, type = "b", pch = 19, 
     col = "red", xlab = "Number of cluster", ylab = "Averaged Log-likelihood", main="Cross-Validation results on clusters")
```

For cross validation score the best cluster number is 6 since it has the lowest score.




## Higher dimensions of wine data set
### 3-Dimensions:


We manually select the columns 2, 4 and 6 and use it in EM algorithm:
```{r}
X_3 = as.matrix(wine[,c(2,4,6)])
```

We will use BIC in the performance analysis because it gave the best result in 2 dimensional dataset:

```{r}
bic_3dim_results = BIC_EM(X_3,  3, 10)
plot(3:10, bic_3dim_results$result, type = "b", pch = 19, 
     col = "red", xlab = "Number of cluster", ylab = "Sum of likelihood", main="BIC results with dim=3")
```

Train the EM with new dimensions

```{r}
params_3f = EM_algorithm(X_3, 3, 1000, 0)
params_3f$prob
```

Plot the data in 3D:
```{r}
cluster_3f_result = EMOrigin.findCluster(params_3f$gamma)
colors <- c("#999997", "#E67F00", "#56B4E7")
colors <- colors[as.numeric(cluster_3f_result)]
scatterplot3d(X_3, pch = 19, color=colors,box=FALSE)
```



Now we plot the clusters with ellipses:

```{r}
plot_cluster_area(X_3[,c(1,2)], params_3f$mu[,c(1,2)], cluster_3f_result)
```


We compute the class Error and adjusted Rand Index:

```{r}
classError(cluster_3f_result, y)
```

```{r}
adjustedRandIndex(cluster_3f_result, y)
```

The number of dimension=3 worked ok actually. The class Error(0.20) and adjusted Rand Index(0.475) values are not bad.

### 4-Dimensions:


We manually select the columns 2, 4, 6 and 8 to use it in EM algorithm:
```{r}
X_4 = as.matrix(wine[,c(2,4,6,8)])
```

We will use BIC in the performance analysis because it gave the best result in 2 dimensional dataset:

```{r}
bic_4dim_results = BIC_EM(X_4,  3, 10)
plot(3:10, bic_4dim_results$result, type = "b", pch = 19, 
     col = "red", xlab = "Number of cluster", ylab = "Sum of likelihood", main="BIC results with Dim=4")
```

Train the EM with new dimensions

```{r}
params_4dim = EM_algorithm(X_4, 3, 1000, 0)
params_4dim$prob
```

The cluster probabilities are 0.4669723 0.2519981 0.2810296 respectively

Plot the data in 3D:
```{r}
cluster_4f_result = EMOrigin.findCluster(params_4dim$gamma)
colors <- c("#999997", "#E67F00", "#56B4E7")
colors <- colors[as.numeric(cluster_4f_result)]
scatterplot3d(X_4, pch = 19, color=colors,box=FALSE)
```



Now we plot the clusters with ellipses:

```{r}
plot_cluster_area(X_4[,c(1,2)], params_4dim$mu[,c(1,2)], cluster_4f_result)
```


We compute the class Error and adjusted Rand Index:

```{r}
classError(cluster_4f_result, y)
```

```{r}
adjustedRandIndex(cluster_4f_result, y)
```

The number of dimension=4's result is worse than dimension 3. The class Error and adjusted Rand Index values are 0.455 and 0.219 respectively.


### 5-Dimensions:


We manually select the columns 2, 4, 6, 8 and 10 to use it in EM algorithm:
```{r}
X_5 = as.matrix(wine[,c(2,4,6,8,10)])
```

We will use BIC in the performance analysis because it gave the best result in 2 dimensional dataset:

```{r}
bic_5dim_results = BIC_EM(X_5,  3, 10)
plot(3:10, bic_5dim_results$result, type = "b", pch = 19, 
     col = "red", xlab = "Number of cluster", ylab = "Sum of likelihood", main="BIC results with Dim=5")
```

Train the EM with new dimensions

```{r}
params_5dim = EM_algorithm(X_5, 3, 1000, 0)
params_5dim$prob
```

Plot the data in 3D:
```{r}
cluster_5f_result = EMOrigin.findCluster(params_5dim$gamma)
colors <- c("#999997", "#E67F00", "#56B4E7")
colors <- colors[as.numeric(cluster_5f_result)]
scatterplot3d(X_5, pch = 19, color=colors,box=FALSE)
```


Now we plot the clusters with ellipses:

```{r}
plot_cluster_area(X_5[,c(1,2)], params_5dim$mu[,c(1,2)], cluster_5f_result)
```


We compute the class Error and adjusted Rand Index:

```{r}
classError(cluster_5f_result, y)
```

```{r}
adjustedRandIndex(cluster_5f_result, y)
```
**Comparison of different dimension's results**

I will put the class error and adjusted rand index values' of different dimensioned datasets into a table and compare the results. Till now we have the EM error results of dimension 3,4 and 5 using *the whole dataset*. We don't have the error values of the dataset with 2 dimensions since we splitted the dataset before into train and test. Therefore let's find the error rates of the EM on *the whole dataset*:

```{r}
X_2 = as.matrix(wine[,c(2,4)])
params_2dim = EM_algorithm(X_2, 3, 1000, 0)
cluster_2f_result = EMOrigin.findCluster(params_2dim$gamma)
print(paste0("Class error of dimension 2 data :", classError(cluster_2f_result, y)))
print(paste0("Adjusted rand index error of dimension 2 data: ", adjustedRandIndex(cluster_2f_result, y)))
```

In below table, you can see the class error and adjusted rand index of EM algortihm using *the whole dataset*.

```{r table2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl <- "
| Dimension of data        | class Error           | Adjusted rand index  |
|---------------|:-------------:|------:|
| 2      | 0.477 | 0.159|
| 3      | 0.20      |   0.475 |
| 4 | 0.455     |    0.219 |
| 5 | 0.460    |    0.213 |
"
cat(tabl) # output the table in a format good for HTML/PDF/docx conversion
```

According to table, the best clustering result is with dimension 3 since it has the lowest class error(0.20) and highest adjusted rand index(0.475).