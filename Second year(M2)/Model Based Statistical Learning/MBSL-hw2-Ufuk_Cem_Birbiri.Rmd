---
title: "Assignment 2"
author: "Ufuk Cem Birbiri"
date: "25.01.2023"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


---------------------------------


# PART-1
### Comparison of imputation methods

Installing libraries
```{r, echo=TRUE, message=FALSE, warnings=FALSE}
#install.packages("VIM")
library(VIM)
data(SBS5242)
XNA <- SBS5242
```
Before starting the questions, let's investigate the dataset.
```{r, echo=TRUE, message=FALSE, warnings=FALSE}
head(XNA)
```
And we can see the dimension:
```{r, echo=TRUE, message=FALSE, warnings=FALSE}
dim(XNA)
```
The dimensoin of the dataset is (262, 9) . We can also display the column names:

```{r, echo=TRUE, message=FALSE, warnings=FALSE}
colnames(XNA)
```

### Question 1
* Visualize the missing values with the function **aggr** of the **VIM** package and compute the global percentage of missing values in the dataset.


The function "aggr" measures the number of missing entries in each feature and for certain combinations of features (which tend to be missing simultaneously).

```{r, echo=TRUE, message=FALSE, warnings=FALSE}
## Finding missing values
a <- aggr(XNA)
```

The graph on rigth shows the proportion of missing values. The graph on the right panel represents the pattern, with blue for observed and red for missing.

The count of missing values and the percantage is shown in below:

```{r, echo=TRUE, message=FALSE, warnings=FALSE}
summary(a)
```

The second table shows the missings in combinations of variables in which 1 is missing, 0 is observed. We see that the combination which is the most frequent is the one where all the variables are observed (221 values, 84.3%). Then, the second one is the one where only the last column("ISACH") is missing (4 rows, 1.52%) 

### Question 2
* Introduce 30\% synthetic missing values in the dataset. You can use the function **synthetic_MCAR** (defined below) or create your own function. Explain why the synthetic missing values introduced as such are MCAR.




 With Mcar function, the values are randomly missing from your dataset. Missing data values do not relate to any other data in the dataset and there is no pattern to the actual values of the missing data themselves.

```{r, echo=TRUE, message=FALSE, warnings=FALSE}
#Introduce 30% synthetic missing values in the dataset.

library(FactoMineR)
n <- dim(XNA)[1]
d <- dim(XNA)[2]

prop.miss <- 0.3 
nb.miss <- floor(n*d*prop.miss)
missing_pattern.mcar <- matrix(runif(n*d,0,1) < 0.3,nrow=n,ncol=d)
XNA.mcar <- XNA
XNA.mcar[missing_pattern.mcar] <- NA
head(XNA.mcar)
XNA.mcar <- as.matrix(as.data.frame(XNA.mcar))
```



### Question 3
* Compare three imputation methods (mean imputation, another single imputation, multiple imputation) by computing the MSE for the synthetic missing values.

#### 3.1 Imputation by the mean



```{r, imputation by the mean}
ImputeMean <- function(tab){
  m <- apply(tab, 2, mean, na.rm = TRUE)
  tab <- sapply(1:ncol(tab), function(x) ifelse(is.na(tab[,x]), m[x], tab[,x]))
  tab <- as.data.frame(tab)
  return(tab)
}

XNA.mean <- ImputeMean(XNA.mcar)
head(XNA.mean)
```

#### 3.2 Imputation with softImpute


For softImpute, the main arguments are the following ones: 

* `x`: the dataset with missing values (matrix).

* `rank.max`: the restricted rank of the solution, which should not be bigger than min(dim(x))-1.

* `lambda`: the nuclear-norm regularization parameter.


```{r, imputation with softImpute}
library(softImpute)
sft <- softImpute(x = XNA.mcar, rank.max = 4, lambda = 0.001)
XNA.sft <- sft$u %*% diag(sft$d) %*% t(sft$v) # compute the factorization
XNA.sft[which(!is.na(XNA.mcar))] <- XNA.mcar[which(!is.na(XNA.mcar))] # replace missing values by computed values
head(XNA.sft)
```


#### 3.3 Imputation with multiple imputation

The parameters of the mice function is:

1.  `data`: the dataset with missing values.

2.  `m`: number of multiple imputations.


```{r, message=FALSE, warnings=FALSE, results='hide'}
library(mice)
nb_imputeddataset <- 5
mice_mice <- mice(data = matrix(XNA.mcar,nrow=n,ncol=d), m = nb_imputeddataset)

```

```{r }
IMP <- 0
for (i in 1:nb_imputeddataset) { IMP <- IMP + mice::complete(mice_mice, i)}
XNA.mice  <-  IMP/nb_imputeddataset  #5 is the default number of multiple imputations
head(XNA.mice)
```



```{r, Mean Squared Error }
MSE <- function(X1, X2){ 
  
  return(mean( (X1 - X2)^2 ,na.rm=T)^2 ) 
  
  }
```


```{r, comparison of the methods}
mse_mean <- MSE(XNA, as.matrix(XNA.mean))
mse_mean
mse_soft <- MSE(XNA, as.matrix(XNA.sft))
mse_soft
mse_mice <- MSE(XNA, as.matrix(XNA.mice))
mse_mice
```

The mean squared error is lowest in multiple imputation method.

### Question 4
* To analyse the stochasticity implied by the introduction of missing values, repeat the procedure (introduction of synthetic missing values and imputation) several times and plot the distribution of the MSEs by showing a boxplot. 

#### 4.1 Repeating the experiments
I created a for-loop below to repeat the experiments 100 times. In each iteration, I introduce the missing values with 30% in the dataset, do the 3 different imputation methods and calculate the MSE, separately for each imputation methods. After this for-loop, I create a data-frame that contains the MSE's for each imputation method. At the end I plot the box-plot of three different imputation methods.

Now let's do the experiment 100 times and save the MSE's:

```{r, message=FALSE, warnings=FALSE, results='hide'}

#30% missing value
prop.miss <- 0.3

mean_impt = c()
soft_impt = c()
multip_impt = c()
index = c()

for (i in 1:100) {

  #Introduce missing values and percentage
  nb.miss <- floor(n*d*prop.miss)
  missing_pattern.mcar <- matrix(runif(n*d,0,1) < 0.3,nrow=n,ncol=d)
  XNA.mcar <- XNA
  XNA.mcar[missing_pattern.mcar] <- NA
  XNA.mcar <- as.matrix(as.data.frame(XNA.mcar))
  
  #Mean imputation:
  XNA.mean <- ImputeMean(XNA.mcar)
  
  #calculate MSE
  mse_mean <- MSE(XNA, as.matrix(XNA.mean))
  mean_impt <- append(mean_impt, mse_mean)
    
  #Soft Imputation:
  sft <- softImpute(x = XNA.mcar, rank.max = 4, lambda = 0.001)
  XNA.sft <- sft$u %*% diag(sft$d) %*% t(sft$v) 
  XNA.sft[which(!is.na(XNA.mcar))] <- XNA.mcar[which(!is.na(XNA.mcar))] 
  
  #calculate MSE
  mse_soft <- MSE(XNA, as.matrix(XNA.sft))
  soft_impt <- append(soft_impt, mse_soft)
  
  #Multiple imputation
  nb_imputeddataset <- 5
  mice_mice <- mice(data = matrix(XNA.mcar,nrow=n,ncol=d), m = nb_imputeddataset)
  IMP <- 0
  for (i in 1:nb_imputeddataset) { IMP <- IMP + mice::complete(mice_mice, i)}
  XNA.mice  <-  IMP/nb_imputeddataset  #5 is the default number of multiple imputations
  
  #calculate MSE
  mse_mice <- MSE(XNA, as.matrix(XNA.mice))
  multip_impt <- append(multip_impt, mse_mice)
  
  #Index:
  #I also save the index to use it in the box-plot later as the x-axis
  index <- append(index, i)
}

```

Create the data-frame:


```{r, message=FALSE, warnings=FALSE, results='hide'}
data <- data.frame(
  name=c( index  ),
  mean_impt=c( mean_impt),
  soft_impt=c( soft_impt),
  multip_impt = c(multip_impt)
)

```
#### 4.2 Mean imputation Box-Plot

Let's plot the mean imputation box-plot

```{r, message=FALSE, warnings=FALSE}
# Libraries
library(tidyverse)
library(hrbrthemes)
library(viridis)



data %>%
  ggplot( aes(x=name, y=mean_impt, fill=name)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = 0, alpha=0.6, option="A") +
    theme_ipsum() +
    theme(
      legend.position="none",
      plot.title = element_text(size=11)
    ) +
    ggtitle("Mean Imput boxplot") +
    xlab("Experiments")+ ylab("MSE")

```

#### 4.3 SOFT imputation Box-Plot

Plot the Soft imputation box-plot.

```{r, message=FALSE, warnings=FALSE}

data %>%
  ggplot( aes(x=name, y=soft_impt, fill=name)) +
    geom_boxplot(color="red", fill="orange") +
    scale_fill_viridis(discrete = 0, alpha=0.6, option="A") +
  
    theme_ipsum() +
    theme(
      legend.position="none",
      plot.title = element_text(size=11)
    ) +
    ggtitle("SOFT Imput boxplot") +
    xlab("Experiments")+ ylab("MSE")

```


#### 4.3 Multiple imputation Box-Plot

At the end, let's plot the multiple imputation box-plot
```{r, message=FALSE, warnings=FALSE}

data %>%
  ggplot( aes(x=name, y=multip_impt, fill=name)) +
    geom_boxplot(color="red", fill="purple") +
    scale_fill_viridis(discrete = 0, alpha=0.6, option="A") +
  
    theme_ipsum() +
    theme(
      legend.position="none",
      plot.title = element_text(size=11)
    ) +
    ggtitle("Multiple Imput boxplot") +
    xlab("Experiments")+ ylab("MSE")

```

# PART-2

## Question 1:
 Let us consider an i.i.d. sample $(y_i,X_i)_{i=1,\dots,n}$. 

$y_i\in \{0,1\}$ is the outcome variable, $X_i \in \mathbb{R}^d$ are the covariates, $\beta \in \mathbb{R}^d$ is the regression parameter. 

We assume the logistic regression: 
$$\mathbb{P}(y_i=1|X_i;\beta)=\frac{1}{1+\exp(-X_i^T\beta)}$$

 In the sequel, we consider that $y$ contains some MCAR values and we derive an EM algorithm to estimate $\beta$.


**Write the observed log-likelihood. Note that in this case, we can maximize it with numerical methods (for example using the R function glm and argument family=binomial).**

## Solution 1:

The parameter to be estimated in the equation of a logistic regression is β vector.

To estimate β vector, we consider the N sample with labels either 0 or 1.

For samples labeled as ‘1’, we try to estimate β such that the product of all probability p(x) is as close to 1 as possible. And for samples labeled as ‘0’, we try to estimate β such that the product of all probability is as close to 0 as possible in other words (1 — p(x)) should be as close to 1 as possible.

This intuition is represented as

$$for\ samples\ labeled\ as\ 1:\prod_{s \in y_i = 1}^{} p(x_i)$$
$$for\ samples\ labeled\ as\ 0: \prod_{s \in y_i = 0}^{} (1- p(x_i))$$

On combining the above conditions we want to find β parameter such that the product of both of these products is maximum over all elements of the dataset.

$$L(\beta) = \prod_{s \in y_i = 1}^{} p(x_i) * \prod_{s \in y_i = 0}^{} (1- p(x_i))$$

This function is the one we need to optimize and is called the likelihood function.

Now, We combine the products and take log-likelihood to simply it further

$$L(\beta) = \prod_{s}^{} p(x_i)^{y_i} * (1- p(x_i))^{1- y_i}$$

$$l(\beta) = \sum_{i=1}^{n} y_i  log(p(x_i)) + \sum_{i=1}^{n} (1- y_i)log(1- p(x_i))$$
where $l(\beta)$ is the log-likelihood. We will call this likelihood as observed log-likelihood and use this symbol: $l_{obs}(\beta)$.

Let’s substitute p(x) with its exponent form

$$l_{obs}(\beta) = \sum_{i=1}^n y_i \log \left(\frac{1}{1+\exp \left(-X_{i}^{T} \beta\right)}\right) + (1-y_i) \log \left(1-\frac{1}{1+\exp \left(-X_{i}^{T} \beta\right)}\right)$$

$$l_{obs}(\beta) = \sum_{i=1}^n y_i \log \left(\frac{1}{1+\exp \left(-X_{i}^{T} \beta\right)}\right) + (1-y_i) \log \left(\frac{exp \left(-X_{i}^{T} \beta\right)}{1+\exp \left(-X_{i}^{T} \beta\right)}\right)$$


$$l_{obs}(\beta) = \sum_{i=1}^n y_i [ \log \left(\frac{1}{1+\exp \left(-X_{i}^{T} \beta\right)}\right) - \log \left(\frac{exp \left(-X_{i}^{T} \beta\right)}{1+\exp \left(-X_{i}^{T} \beta\right)}\right) ]+ \log \left(\frac{exp \left(-X_{i}^{T} \beta\right)}{1+\exp \left(-X_{i}^{T} \beta\right)}\right)$$


$$l_{obs}(\beta) = \sum_{i=1}^n y_i \beta x_i + \log \left(\frac{1}{1+\exp \left(-X_{i}^{T} \beta\right)}\right)$$

Now we end up with the final form of the log-likelihood function which is to be optimized:

$$l_{obs}(\beta) = \sum_{i=1}^n y_i \beta x_i - \log \left(1+\exp \left(-X_{i}^{T} \beta\right)\right)$$


## Question 2
**Although maximizing the observed likelihood can be done easily, we will also derive an EM algorithm to maximize it. Write the full log-likelihood.**

## Solution 2
We have a dataset with observed and missing values. The full log-likelihood in Logistic Regression model using this kind of data can be calculated with the joint probability of the both observed and missing data since we will be using both distributions. This joint probability can be represented as $P(y_i, R_i | X_i, \beta)$. Here the $R_i$ and the $\beta$ are covariates and the parameters to be estimated. 

We need to calculate the log-likelihood of the all data points so we need to sum all these probabilities inside a log function, i.e:


$$
L(\beta) = \sum_{i=1}^n \log \left(P(y_i, R_i | X_i, \beta)\right)
$$

 We need to put the joint probability that is given in Question 1:
$$\mathbb{P}(y_i=1|X_i;\beta)=\frac{1}{1+\exp(-X_i^T\beta)}$$


Now, we know how to full log-likelihood of the logistic regression. We combine the two formulas:
$$
L(\beta) = \sum_{i=1}^{m} y_i (X_i^T \beta) - \log(1 + \exp(X_i^T \beta)) + \sum_{i=m+1}^{n} \mathbb{P}(y_i = 1 \mid X_i, \beta) \log\mathbb{P}(y_i = 1 \mid X_i, \beta) + (1-\mathbb{P}(y_i = 1 \mid X_i, \beta)) \log(1-\mathbb{P}(y_i = 1 \mid X_i, \beta))
$$

Where the first term is for observed samples and the second is for missing values. ,Later we will calculate the full log-likelihood using EM algorithm.


## Question 3:
 **Show that the E-step can be written as follows**
 $$
Q\left(\beta ; \beta^{(r)}\right)=\sum_{i=1}^{n} Q_{i}\left(\beta ; \beta^{(r)}\right)
$$
 
 where
 $$
Q_{i}\left(\beta ; \beta^{(r)}\right)= \begin{cases}\sum_{y_{i} \in\{0,1\}} p\left(y_{i} \mid x_{i} ; \beta^{(r)}\right) \log p\left(y_{i} \mid x_{i} ; \beta\right) & \text { if } y_{i} \text { is missing } \\ \log p\left(y_{i} \mid x_{i} ; \beta\right) & \text { otherwise. }\end{cases}
$$


## Solution 3:
There are two steps in the EM algorithm.
The E-step basically calculates the expected full log-likelihood. 

We found the full log-likelihood in the previous question and we will that again:
$$
 L(\beta) = \sum_{i=1}^n \log \left(P(y_i, R_i | X_i, \beta)\right)
$$

Since it is a joint probability we can modify it like this:
$$
 = \sum_{i=1}^n \log \left(\mathbb{P}(y_i | X_i, \beta) P(R_i | y_i, X_i, \beta)\right)
$$


$$
 = \sum_{i=1}^n \log \left(\sum_{y_i \in {0,1}} P(y_i | X_i, \beta) P(R_i | y_i, X_i, \beta)\right)
$$


$$
 = \sum_{i=1}^n \sum_{y_i \in {0,1}} \mathbb{P}(y_i | X_i, \beta^{(r)}) \log \left(\mathbb{P}(y_i | X_i, \beta) \mathbb{P}(R_i | y_i, X_i, \beta)\right)
$$

Here we need to consider what is $R_i$. If the $R_i = 0$ then it will be eliminated since it is zero. Therefore we will be usingcase only where $R_i = 1$:


$$for\ samples\ where\ R_i = 1:P(R_i | y_i, X_i, \beta) = 1$$
$$for\ samples\ where\ R_i = 0:P(R_i | y_i, X_i, \beta) = 0$$

So we can write the expected full log-likelihood as:

$$
 L(\beta) = \sum_{i=1}^n \sum_{y_i \in {0,1}}[R_i=1]P(y_i | X_i, \beta^{(r)}) \log \left(P(y_i | X_i, \beta)\right)
$$

--------------------------------------

Let's remember the $Q\left(\beta ; \beta^{(r)}\right)$ function given in the question:


 $$
Q\left(\beta ; \beta^{(r)}\right)=\sum_{i=1}^{n} Q_{i}\left(\beta ; \beta^{(r)}\right)
$$
 
 where
 
 $$
Q_{i}\left(\beta ; \beta^{(r)}\right)= \begin{cases}\sum_{y_{i} \in\{0,1\}} p\left(y_{i} \mid x_{i} ; \beta^{(r)}\right) \log p\left(y_{i} \mid x_{i} ; \beta\right) & \text { if } y_{i} \text { is missing } \\ \log p\left(y_{i} \mid x_{i} ; \beta\right) & \text { otherwise. }\end{cases}
$$



We arrived the same $Q\left(\beta ; \beta^{(r)}\right)$ function by calculating the expected full log-likelihood.


## Question 4:
## 4.1 Code the EM algorithm.
 
 Hint 1: remark that $Q\left(\beta ; \beta^{(r)}\right)$ can be seen as a weighted complete data log-likelihood $\left(\sum_{i=1}^{n} \sum_{k=1}^{n_{i}} \omega_{y_{k}} \log p\left(y_{k} \mid x_{k} ; \beta\right)\right)$ based on a sample size $N=\sum_{i=1}^{n} n_{i}$, where $n_{i}=2$ if $y_{i}$ is missing and $n_{i}=1$ otherwise. The weights are $\omega_{y_{k}}=p\left(y_{k} \mid x_{k} ; \beta^{(r)}\right)$ if $y_{k}$ is missing and $\omega_{y_{k}}=1$ if $y_{k}$ is observed.
 
 Hint 2: For the M-step, you can simply use the function glm with the argument weights.



```{r, message=FALSE, warnings=FALSE}
EM_algorithm <- function(data, labels) {
  #We will initialize the weights accroding to whether the missing data exists or not
  missing_data <- is.na(y)
  
  # calculate the initial weights
  weights <- ifelse(missing_data, 1/mean(missing_data), 1/(1-mean(missing_data)))
  
  #Let's initialize parameter estimator
  d= ncol(X)
  beta_hat <- rep(0, d )
  
  # EM algorithm
  for (i in 1:100) {
    #We will use the glm function here:
    EM_result <- glm(y ~ X, family = binomial(), data = data.frame(X, y), weights = weights)
    #Define the new beta estimator
    beta_hat_new <- coef(EM_result)
    
    #If the difference between the beta_hat and the new beta_hat is less than a threshold(1e-5 in this case), then break. Else, we update the beta_hat:
    if (max(abs(beta_hat_new - beta_hat)) < 1e-5) {
      break}
    
    #Update beta_hat:
    beta_hat <- beta_hat_new
  }
  
  return(beta_hat)
}
```
## 4.2 Apply the EM algorithm for the synthetic data defined below. 
Compare the following estimators for β
by computing the MSE: (i) without missing values (y, X), (ii) with missing values (yNA, X) by using
only the rows which do not contain missing values, (iii) with missing values (yNA, X) by using the EM
algorithm. Note that in (i) and (ii), you just have to use the function glm. Here, you can consider that
the intercept is null. What do we notice for estimators (ii) and (iii) ?

#### Create dataset to use it in EM:
First we create the dataset. This part was given in the Homework pdf.
```{r, message=FALSE, warnings=FALSE}
library(mvtnorm)
set.seed(1)
d <- 3
beta_true <- c(0.1,0.5,0.9)
n <- 1000
mu <- rep(0,d)
Sigma <- diag(1,d)+matrix(0.5,d,d)
X <- rmvnorm(n, mean=mu, sigma=Sigma) #multivariate Gaussian variable
logit_link <- 1/(1+exp(-X%*%beta_true))
y <- (runif(n)<logit_link)*1
#### Introduction of MCAR values
nb_missingvalues <- 0.3*n*d

missing_idx <- sample(1:n,nb_missingvalues)
yna <- y
yna[missing_idx] <- NA
```

### (i) without missing values (y, X)
Above we define missing values in y that is called yna. Since the questions asks for non-missing values, we will use only y.
```{r, message=FALSE, warnings=FALSE}
result_i <- EM_algorithm(X, y) 
result_i
```

### (ii) with missing values (yNA, X) by using only the rows which do not contain missing values
I didn't get this question very well. With missing values by only using rows that don't contain missing values? As I understand correctly, we need to remove the rows that have missing values in both X and y. Then run the EM. 
```{r, message=FALSE, warnings=FALSE}
#yna has the missing values. So we need to check yna to detect missing values:
no_missing <- !is.na(yna)
#Then we only take the parts where there is no missing value in X and y
X_no_missing <- X[no_missing,]
y_no_missing <- yna[no_missing]

#Run the EM
result_ii <- EM_algorithm(X_no_missing, y_no_missing)
result_ii
```



### (iii) with missing values (yNA, X)
I think here we need to use the yna and X without removing any missing values:

```{r, message=FALSE, warnings=FALSE}
result_iii <- EM_algorithm(X, yna)
result_iii
```

As we did in the PART-1 of this homework, we can compare the MSE of our estimators and decide which one works better. I defined the MSE function in the PART-1, therefore I will use it directly.

```{r, Mean Squared Errorr }
MSE <- function(X1, X2){ 
  
  return(mean( (X1 - X2)^2 ,na.rm=T)^2 ) 
  
  }
```

```{r, message=FALSE, warnings=FALSE}

MSE_result_i = MSE(result_i, beta_true)
MSE_result_i

MSE_result_ii = MSE(result_ii, beta_true)
MSE_result_ii

MSE_result_iii = MSE(result_iii, beta_true)
MSE_result_iii



```


* **What do we notice for estimators (ii) and (iii)**

The estimators for (ii) and (iii) are same. Also the MSE are same.

## 4.2 Apply the EM algorithm for the cancer prostate dataset (only quantitative variables) and compare the different estimators for $\beta$. Here, consider that the intercept is not null.
Here we will use the prostate cancer dataset and apply the EM algorithm same as before. At the beginnign we will emove the string valued columns.
```{r}
set.seed(3)

#Load the dataset
cancer_data <- read.table(file = 'cancerprostate.csv',header = T, sep = ";")
#Define the beta_true again
beta_true <- c(0.1,0.5,0.9)
#Eliminate the string feautres
X <- data.frame(cancer_data, stringsAsFactors = 0)
y <- cancer_data$Y
#Apply the EM
#We will write to the glm algorithm's first parameter as [y ~ column_names]
result <- glm(y ~ age + acide + rayonx + taille + log.acid, family = binomial(), data = X)
beta_hat<- coef(result)

#result = EM_algorithm(X, y)
cancer_mse <- MSE(beta_hat, beta_true)
cancer_mse
```
MSE of the cancer dataset is pretty high(2031,125)

## 4.3 Briefly discuss the link with semi-supervised learning

The Expectation-Maximization (EM) algorithm is a technique for finding maximum likelihood estimates of parameters in statistical models, where the data may be incomplete or have missing values. In the context of semi-supervised learning, EM can be used to estimate the parameters of a model when only a portion of the data is labeled.

The EM algorithm works by alternating between two steps: the expectation step (E-step), where the missing data is estimated based on the current parameter estimates, and the maximization step (M-step), where the parameters are updated based on the estimated missing data.

In semi-supervised learning, the EM algorithm can be used to estimate the parameters of a model when only a portion of the data is labeled. The labeled data is used in the M-step to update the model parameters, while the unlabeled data is used in the E-step to estimate the missing labels, which are then used in the M-step to further update the model parameters. This process is repeated until the model parameters converge.

In this way, the EM algorithm can be used to leverage the information in both labeled and unlabeled data to improve the performance of the model, which is particularly useful in situations where labeled data is scarce.